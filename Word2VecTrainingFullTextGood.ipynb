{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import expit as sigmoid \n",
    "import os\n",
    "\n",
    "import re\n",
    "import fitz  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [18, 19, 28, 29]\n",
    "\n",
    "\n",
    "def splitIntoSets(code):\n",
    "    \n",
    "    csv_path = f\"{code}_17 to 39.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    testingSetSize = round(0.1 * total_rows)\n",
    "    remainingSize = total_rows - testingSetSize\n",
    "\n",
    "\n",
    "    testingSet = df.iloc[:testingSetSize]['Bill Id'].tolist()\n",
    "\n",
    "\n",
    "    remaining_df = df.iloc[testingSetSize:].sample(frac=1, random_state=42) \n",
    "    \n",
    "    validationSet = remaining_df.iloc[:testingSetSize]['Bill Id'].tolist()\n",
    "\n",
    "\n",
    "    trainingSet = remaining_df.iloc[testingSetSize:]['Bill Id'].tolist()\n",
    "    \n",
    "    return trainingSet, testingSet, validationSet\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import re\n",
    "\n",
    "def extract_clean_flat_text(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        if doc.page_count > 2:\n",
    "            pages = [doc[i].get_text() for i in range(1, doc.page_count - 1)]\n",
    "        else:\n",
    "            pages = []\n",
    "        \n",
    "        doc.close()  \n",
    "\n",
    "        full_text = \" \".join(pages)\n",
    "        full_text = full_text.replace(\"\\n\", \" \").strip()\n",
    "        full_text = re.sub(r\"\\([^)]*\\)\", \"\", full_text)  \n",
    "        full_text = re.sub(r\"\\b\\d+\\b\", \"\", full_text)  \n",
    "        full_text = re.sub(r\"[^a-zA-Z.,; ]\", \" \", full_text)  \n",
    "        full_text = re.sub(r\"\\s+\", \" \", full_text)  \n",
    "        return full_text.lower().strip()\n",
    "\n",
    "    except fitz.EmptyFileError:\n",
    "        print(f\"Warning: The file {pdf_path} is empty and cannot be processed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {pdf_path}: {e}\")\n",
    "\n",
    "    return \"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codes = [29]\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def count_sentences(text):\n",
    "    return len(re.split(r'(?<=[.!?]) +', text))\n",
    "\n",
    "def makeDataSets(codes):\n",
    "    total_sentences = 0\n",
    "    sentence_counts = {'training': 0, 'testing': 0, 'validation': 0}\n",
    "    \n",
    "    for code in codes:\n",
    "        inBasePath = r\"C:\\Users\\ander\\Downloads\\MLP\\billTextDownload\"\n",
    "        inPath = os.path.join(inBasePath, f\"{code}_17 to 39\")\n",
    "        \n",
    "        outputBase = r\"C:\\Users\\ander\\Downloads\\MLP\\cleanedTexWithID\"\n",
    "        outputPath = os.path.join(outputBase, f\"{code}\")\n",
    "        os.makedirs(outputPath, exist_ok=True)\n",
    "        \n",
    "        trainingIndexes, testingIndexes, validationIndexes = splitIntoSets(code)\n",
    "        training_text_path = os.path.join(outputPath, \"Training\", f\"training_text{code}.csv\")\n",
    "        os.makedirs(os.path.join(outputPath, \"Training\"), exist_ok=True)\n",
    "        \n",
    "        with open(training_text_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['id', 'extracted_text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for billID in trainingIndexes:\n",
    "                for filename in os.listdir(inPath):\n",
    "                    if filename.endswith(\".pdf\"):\n",
    "                        bill_number = filename[:-4].split('_')[-1]\n",
    "                        try:\n",
    "                            bill_number = int(bill_number)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        if bill_number == billID:\n",
    "                            pdf_path = os.path.join(inPath, filename)\n",
    "                            extracted_text = extract_clean_flat_text(pdf_path)\n",
    "                            if extracted_text:\n",
    "                                sentence_count = count_sentences(extracted_text)\n",
    "                                sentence_counts['training'] += sentence_count\n",
    "                                total_sentences += sentence_count\n",
    "                                writer.writerow({'id': bill_number, 'extracted_text': extracted_text})\n",
    "\n",
    "        validation_text_path = os.path.join(outputPath, \"Validation\", f\"validation_text{code}.csv\")\n",
    "        os.makedirs(os.path.join(outputPath, \"Validation\"), exist_ok=True)\n",
    "        \n",
    "        with open(validation_text_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['id', 'extracted_text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for billID in validationIndexes:\n",
    "                for filename in os.listdir(inPath):\n",
    "                    if filename.endswith(\".pdf\"):\n",
    "                        bill_number = filename[:-4].split('_')[-1]\n",
    "                        try:\n",
    "                            bill_number = int(bill_number)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        if bill_number == billID:\n",
    "                            pdf_path = os.path.join(inPath, filename)\n",
    "                            extracted_text = extract_clean_flat_text(pdf_path)\n",
    "                            if extracted_text:\n",
    "                                sentence_count = count_sentences(extracted_text)\n",
    "                                sentence_counts['validation'] += sentence_count\n",
    "                                total_sentences += sentence_count\n",
    "                                writer.writerow({'id': bill_number, 'extracted_text': extracted_text})\n",
    "\n",
    "        testing_text_path = os.path.join(outputPath, \"Testing\", f\"testing_text{code}.csv\")\n",
    "        os.makedirs(os.path.join(outputPath, \"Testing\"), exist_ok=True)\n",
    "        \n",
    "        with open(testing_text_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['id', 'extracted_text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for billID in testingIndexes:\n",
    "                for filename in os.listdir(inPath):\n",
    "                    if filename.endswith(\".pdf\"):\n",
    "                        bill_number = filename[:-4].split('_')[-1]\n",
    "                        try:\n",
    "                            bill_number = int(bill_number)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        if bill_number == billID:\n",
    "                            pdf_path = os.path.join(inPath, filename)\n",
    "                            extracted_text = extract_clean_flat_text(pdf_path)\n",
    "                            if extracted_text:\n",
    "                                sentence_count = count_sentences(extracted_text)\n",
    "                                sentence_counts['testing'] += sentence_count\n",
    "                                total_sentences += sentence_count\n",
    "                                writer.writerow({'id': bill_number, 'extracted_text': extracted_text})\n",
    "\n",
    "    print(\"Total sentences across all datasets:\", total_sentences)\n",
    "    print(\"Sentences in Training data:\", sentence_counts['training'])\n",
    "    print(\"Sentences in Testing data:\", sentence_counts['testing'])\n",
    "    print(\"Sentences in Validation data:\", sentence_counts['validation'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The file C:\\Users\\ander\\Downloads\\MLP\\billTextDownload\\19_17 to 39\\363.pdf is empty and cannot be processed.\n",
      "Total sentences across all datasets: 329300\n",
      "Sentences in Training data: 244006\n",
      "Sentences in Testing data: 39003\n",
      "Sentences in Validation data: 46291\n"
     ]
    }
   ],
   "source": [
    "makeDataSets([29, 28, 18, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 Fail\n",
    "# 2 Success\n",
    "# 8 Commons\n",
    "# 9 Lords\n",
    "#18 failed commons\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences\n",
    "\n",
    "def splitDocument(house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}/Training/training_text{code}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "# returns a list of lists . List each sentnece, and inside there is a list of words for each sentence\n",
    "\n",
    "\n",
    "def splitDocumentALL(startSession, endSession, house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}_{startSession} to {endSession} fullText.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# input (sentence: list of words, model: gensim model, window: window= windowSize of word2vec, \n",
    "#debug: print intermediate calculations for debugging)\n",
    "\n",
    "def score_sentence(sentence, model, window=7, debug=False):\n",
    "    log_prob = 0.0 # total log prob for the sentence\n",
    "    sentence_length = len(sentence)\n",
    "    word_pair_probs = []  \n",
    "\n",
    "    # Code for equation 1 \n",
    "    for index, center_word in enumerate(sentence):\n",
    "        if center_word not in model.wv:\n",
    "            if debug:\n",
    "                print(f\"Center word '{center_word}' not in vocabulary.\")\n",
    "            continue\n",
    "        center_vector = model.wv[center_word]\n",
    "\n",
    "        start = max(0, index - window)\n",
    "        end = min(sentence_length, index + window + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == index:\n",
    "                continue\n",
    "            context_word = sentence[j]\n",
    "            if context_word not in model.wv:\n",
    "                if debug:\n",
    "                    print(f\"Context word '{context_word}' not in vocabulary.\")\n",
    "                continue\n",
    "            context_vector = model.wv[context_word]\n",
    "\n",
    "            dot_product = np.dot(center_vector, context_vector)\n",
    "            prob = sigmoid(dot_product)\n",
    "\n",
    "            word_pair_probs.append((center_word, context_word, prob))\n",
    "\n",
    "            log_prob += np.log(prob + 1e-10)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- Word Pair Probabilities ---\")\n",
    "        for center, context, prob in word_pair_probs:\n",
    "            print(f\"p({context} | {center}) = {prob:.6f}\")\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score an entire document (S sentences) under all models (Equation 2)\n",
    "# input (sentencces:  a list of sentences ,models: the dictionary of models, window: the window size for score sentences)\n",
    "# outpur: a sentences x categories (failed , succesful ....) with eahc sentence score according to score_sentence\n",
    "\n",
    "def score_document(sentences, models, window=5):\n",
    "    \"\"\"\n",
    "    Compute the score x category matrix of sentence scores for a document.\n",
    "    \n",
    "    sentences: list of sentences, each sentence is a list of words\n",
    "    models: dict of {category: Word2Vec model}\n",
    "    \"\"\"\n",
    "    S = len(sentences)\n",
    "    C = len(models)\n",
    "    \n",
    "    sentence_scores = np.zeros((S, C))\n",
    "    \n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        for c_idx, (category, model) in enumerate(models.items()):\n",
    "            sentence_scores[s_idx, c_idx] = score_sentence(sentence, model, window)\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "# calculate document probabilities (Equation 5)\n",
    "\n",
    "# input: the sxc array\n",
    "# output: a 1x cateories array with the average score for all sentences in document \n",
    "def document_probabilities(sentence_scores):\n",
    "\n",
    "    return sentence_scores.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# compute class probabilities ( Equation 3)\n",
    "\n",
    "# input:  the array from document_probabilities\n",
    "#ouput: normalized probabilities after bayes rule is applied #todo: change the priors to correspond to each class \n",
    "def class_probabilities(log_doc_probs):\n",
    "    num_classes = len(log_doc_probs)\n",
    "    doc_probs = np.exp(log_doc_probs - np.max(log_doc_probs))  # Use log-sum-exp for numerical stability\n",
    "    priors = np.ones(num_classes) / num_classes\n",
    "    numerator = doc_probs * priors\n",
    "    denominator = np.sum(numerator)\n",
    "    probs = numerator / denominator\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "# classify the document (Equation 6)\n",
    "# checks which of the numbers in the 1d array from document probabilities (the average across the classes ) is biggest and returns the index and array (for debuging) \n",
    " \n",
    "def classify_document(sentence_scores):\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    class_probs = class_probabilities(doc_probs)\n",
    "    predicted_class_idx = np.argmax(class_probs)\n",
    "    return predicted_class_idx, doc_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read FilecleanedTextFull/18_17 to 39 fullText.txt\n",
      "Read FilecleanedTextFull/19_17 to 39 fullText.txt\n",
      "Read FilecleanedTextFull/28_17 to 39 fullText.txt\n",
      "Read FilecleanedTextFull/29_17 to 39 fullText.txt\n",
      "Sentences retreived\n",
      "259841\n",
      "Read FilecleanedTextFull/18/Training/training_text18.txt\n",
      "Read FilecleanedTextFull/19/Training/training_text19.txt\n",
      "Read FilecleanedTextFull/28/Training/training_text28.txt\n",
      "Read FilecleanedTextFull/29/Training/training_text29.txt\n",
      "Sentences per category\n",
      "Sentences in FailedCommons 41666.0\n",
      "Sentences in FailedCommons 21913.0\n",
      "Sentences in FailedCommons 157403.0\n",
      "Sentences in FailedCommons 21045.0\n",
      "FailedCommons\n",
      "FailedLords\n",
      "SuccesCommons\n",
      "SuccessLords\n",
      "dict_keys(['FailedCommons', 'FailedLords', 'SuccesCommons', 'SuccessLords'])\n"
     ]
    }
   ],
   "source": [
    "startSession = 17\n",
    "endSession= 39\n",
    "\n",
    "\n",
    "\n",
    "#combine sentences from all 4 files into 1 list of lists. Each sentence a list of words \n",
    "allSentences = [sentence for listOfSentneces in splitDocumentALL(startSession, endSession) for sentence in listOfSentneces]\n",
    "\n",
    "print(\"Sentences retreived\")\n",
    "print(len(allSentences))\n",
    "\n",
    "houseDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "[g for g in houseDictionary]\n",
    "\n",
    "\n",
    "\n",
    "# populate a dictionary: key: category of bill, value: list of lists with sentences from bills of that category(FailedCommons, etc)\n",
    "categorizedBillSentences = {key: [sentence for listOfSentneces in splitDocument(houseDictionary[key]) for sentence in listOfSentneces] for key in houseDictionary}\n",
    "\n",
    "# creates series with how many sentnces for each catgory(debugging)\n",
    "numberBills = pd.Series({key: len(categorizedBillSentences[key]) for key in houseDictionary}, dtype=\"float64\" )\n",
    "\n",
    "print(\"Sentences per category\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[0]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[1]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[2]}\")\n",
    "print(f\"Sentences in FailedCommons {numberBills[3]}\")\n",
    "\n",
    "\n",
    "# populate houseDictionary with bills of appropaite category as lists of lists\n",
    "for key in houseDictionary:\n",
    "    for i in range(len(categorizedBillSentences[key])):\n",
    "        categorizedBillSentences[key][i] = [word for word in categorizedBillSentences[key][i]]\n",
    "\n",
    "\n",
    "#creates a dictionary of word2vec models initialized with the vocabulary of all sentences         \n",
    "\n",
    "\n",
    "models = { }\n",
    "\n",
    "\n",
    "def trainW2V(key, T=25):\n",
    "    sentences = categorizedBillSentences[key]\n",
    "    for epoch in range(T):\n",
    "        print(f\"{epoch}\", end=\"\")\n",
    "        np.random.shuffle(sentences)\n",
    "        models[key].train(sentences, total_examples=len(sentences), epochs=1)\n",
    "        models[key].alpha *= 0.9\n",
    "        models[key].min_alpha = models[key].alpha\n",
    "    print(\".\")\n",
    "\n",
    "\n",
    "for key in houseDictionary:\n",
    "    models[key] = Word2Vec(allSentences, workers=4, hs=1, sg=1, negative=0, min_count=7, vector_size =100,window = 7 )\n",
    "    models[key].build_vocab(allSentences) \n",
    "    \n",
    "    \n",
    "\n",
    "# trains each of the initialized models only with text from each specific category. 4 models trained with different bill text\n",
    "#for key in houseDictionary:\n",
    " #   print(key, end=\":\")\n",
    "  #  trainW2V(key)\n",
    "\n",
    "\n",
    "#####\n",
    "for key in houseDictionary:\n",
    "    print (key)\n",
    "\n",
    "print(models.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainW2V (\"SuccesCommons\")\n",
    "\n",
    "modelToSave = models[\"SuccesCommons\"]\n",
    "modelToSave.save(\"Model_28_re_trained_vs_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model in models.items():\n",
    "    num_words = len(model.wv.key_to_index)\n",
    "    print(f\"{key} has learned {num_words} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualizeModel(model, key, top_n=20):\n",
    "  \n",
    "    if not hasattr(model, 'wv') or not hasattr(model.wv, 'index_to_key'):\n",
    "        raise ValueError(\"Wrong model .\")\n",
    "    \n",
    "\n",
    "    top_words = model.wv.index_to_key[:top_n] \n",
    "    top_word_vectors = model.wv[top_words]  \n",
    "    \n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(top_word_vectors)\n",
    "    \n",
    "  \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(result[:, 0], result[:, 1])\n",
    "    \n",
    "\n",
    "    for i, word in enumerate(top_words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} Word Embeddings Visualization for {key}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "for key , model in models.items():\n",
    "    visualizeModel(model, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=12578, vector_size=300, alpha=0.022500000000000003>\n",
      "Word2Vec<vocab=12578, vector_size=300, alpha=0.022500000000000003>\n",
      "Word2Vec<vocab=12578, vector_size=300, alpha=0.022500000000000003>\n",
      "Word2Vec<vocab=12578, vector_size=300, alpha=0.022500000000000003>\n",
      "FailedCommons\n",
      "FailedLords\n",
      "SuccesCommons\n",
      "SuccessLords\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
