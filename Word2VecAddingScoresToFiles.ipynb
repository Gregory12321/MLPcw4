{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import expit as sigmoid \n",
    "import os\n",
    "import re\n",
    "import fitz  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Function Definitions \n",
    "\n",
    "# 1 Fail\n",
    "# 2 Success\n",
    "# 8 Commons\n",
    "# 9 Lords\n",
    "#18 failed commons\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences\n",
    "\n",
    "def splitDocument(house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}/Training/training_text{code}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "# returns a list of lists . List each sentnece, and inside there is a list of words for each sentence\n",
    "\n",
    "\n",
    "def splitDocumentALL(startSession, endSession, house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}_{startSession} to {endSession} fullText.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# input (sentence: list of words, model: gensim model, window: window= windowSize of word2vec, \n",
    "#debug: print intermediate calculations for debugging)\n",
    "\n",
    "def score_sentence(sentence, model, window=7, debug=False):\n",
    "    log_prob = 0.0 # total log prob for the sentence\n",
    "    sentence_length = len(sentence)\n",
    "    word_pair_probs = []  \n",
    "\n",
    "    # Code for equation 1 \n",
    "    for index, center_word in enumerate(sentence):\n",
    "        if center_word not in model.wv:\n",
    "            if debug:\n",
    "                print(f\"Center word '{center_word}' not in vocabulary.\")\n",
    "            continue\n",
    "        center_vector = model.wv[center_word]\n",
    "\n",
    "        start = max(0, index - window)\n",
    "        end = min(sentence_length, index + window + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == index:\n",
    "                continue\n",
    "            context_word = sentence[j]\n",
    "            if context_word not in model.wv:\n",
    "                if debug:\n",
    "                    print(f\"Context word '{context_word}' not in vocabulary.\")\n",
    "                continue\n",
    "            context_vector = model.wv[context_word]\n",
    "\n",
    "            dot_product = np.dot(center_vector, context_vector)\n",
    "            prob = sigmoid(dot_product)\n",
    "\n",
    "            word_pair_probs.append((center_word, context_word, prob))\n",
    "\n",
    "            log_prob += np.log(prob + 1e-10)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- Word Pair Probabilities ---\")\n",
    "        for center, context, prob in word_pair_probs:\n",
    "            print(f\"p({context} | {center}) = {prob:.6f}\")\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score an entire document (S sentences) under all models (Equation 2)\n",
    "# input (sentencces:  a list of sentences ,models: the dictionary of models, window: the window size for score sentences)\n",
    "# outpur: a sentences x categories (failed , succesful ....) with eahc sentence score according to score_sentence\n",
    "\n",
    "def score_document(sentences, models, window=5):\n",
    "    \"\"\"\n",
    "    Compute the score x category matrix of sentence scores for a document.\n",
    "    \n",
    "    sentences: list of sentences, each sentence is a list of words\n",
    "    models: dict of {category: Word2Vec model}\n",
    "    \"\"\"\n",
    "    S = len(sentences)\n",
    "    C = len(models)\n",
    "    \n",
    "    sentence_scores = np.zeros((S, C))\n",
    "    \n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        for c_idx, (category, model) in enumerate(models.items()):\n",
    "            sentence_scores[s_idx, c_idx] = score_sentence(sentence, model, window)\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "# calculate document probabilities (Equation 5)\n",
    "\n",
    "# input: the sxc array\n",
    "# output: a 1x cateories array with the average score for all sentences in document \n",
    "def document_probabilities(sentence_scores):\n",
    "\n",
    "    return sentence_scores.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# compute class probabilities ( Equation 3)\n",
    "\n",
    "# input:  the array from document_probabilities\n",
    "#ouput: normalized probabilities after bayes rule is applied #todo: change the priors to correspond to each class \n",
    "def class_probabilities(log_doc_probs):\n",
    "\n",
    "    num_classes = len(log_doc_probs)\n",
    "    doc_probs = np.exp(log_doc_probs - np.max(log_doc_probs))  \n",
    "    priors = np.ones(num_classes) / num_classes\n",
    "    numerator = doc_probs * priors\n",
    "    denominator = np.sum(numerator)\n",
    "    probs = numerator / denominator\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "# classify the document (Equation 6)\n",
    "# checks which of the numbers in the 1d array from document probabilities (the average across the classes ) is biggest and returns the index and array (for debuging) \n",
    " \n",
    "def classify_document(sentence_scores):\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    class_probs = class_probabilities(doc_probs)\n",
    "    predicted_class_idx = np.argmax(class_probs)\n",
    "    return predicted_class_idx, class_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained models \n",
    "\n",
    "def scoreSentences(listOfSentences, modelsList, predictedCategory):\n",
    "    success= 0\n",
    "    fail =0\n",
    "    \n",
    "    categories = list(modelsList.keys())\n",
    "    \n",
    "    for i, sentence_text in enumerate(listOfSentences, 1):\n",
    "        document = splitSentences(sentence_text)\n",
    "        sentence_scores = score_document(document, modelsList, window=5)\n",
    "        doc_probs = document_probabilities(sentence_scores)\n",
    "        probs = class_probabilities(doc_probs)\n",
    "        predicted_idx, doc_probs = classify_document(sentence_scores)\n",
    "        \n",
    "        print(f\" Predicting Sentence {document}\")\n",
    "        print(f\"\\nSentence {i}:\")\n",
    "        print(f\"Predicted class: {categories[predicted_idx]}\")\n",
    "        print(f\"Document probabilities: {doc_probs}\")\n",
    "        print(f\"Class probabilities: {probs}\")\n",
    "        \n",
    "        predicted_class = categories[predicted_idx]\n",
    "        \n",
    "        if (predicted_class == predictedCategory or\n",
    "            (predicted_class in [\"FailedCommons\", \"FailedLords\"] and \n",
    "             predictedCategory in [\"FailedCommons\", \"FailedLords\"])):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    total = fail + success\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = success / total\n",
    "        print(f\"Correct Prediction: {accuracy}\")\n",
    "    else:\n",
    "        print(\"Error in predicition.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models and populate models dictionar\n",
    "\n",
    "model_18 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\FailedCommons_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_19 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\FailedLords_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_29 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\SuccessLords_word2vec_workers=4, hs=1, sg=1, negative=0, min_count=10, vector_size =300,window = 7.model\")\n",
    "model_28 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\Model_28_re_trained_vs_100\")\n",
    "\n",
    "\n",
    "\n",
    "houseDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "\n",
    "models ={}\n",
    "\n",
    "for key, indices in houseDictionary.items():\n",
    "    for index in indices:\n",
    "        if index == 18:\n",
    "            models[key] = model_18\n",
    "        elif index == 19:\n",
    "            models[key] = model_19\n",
    "        elif index == 28:\n",
    "            models[key] = model_28\n",
    "        elif index == 29:\n",
    "            models[key] = model_29\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to compute the scores for the sentences per individual cell and add the whole array \n",
    "#as well as the code for the actual predicted score \n",
    "from tqdm import tqdm\n",
    "def scoreDocumentCSV(text, models):\n",
    "    filtered_sentences = splitSentences(text)\n",
    "    sentence_scores = score_document(filtered_sentences, models, window=7)\n",
    "    index, doc_probs = classify_document(sentence_scores)\n",
    "    \n",
    "    return index, doc_probs\n",
    "        \n",
    "\n",
    "def appendScoresToFile(df, models, path):\n",
    "    df['predicted_class'] = None\n",
    "    df['doc_probs'] = None\n",
    "    categories = list(models.keys())\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        result_index, probs_array = scoreDocumentCSV(row['extracted_text'], models)\n",
    "        \n",
    "        df.at[index, 'predicted_class'] = categories[result_index]\n",
    "        df.at[index, 'doc_probs'] = probs_array\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\miniconda3\\envs\\pdiot\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 53/53 [00:58<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|███████████████████████████████████████████████████████████████| 132/132 [01:58<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "## Score the training\n",
    "remaining = [29, 28, 18, 19]\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\cleanedTexWithID\" \n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Testing\\\\testing_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)  \n",
    "    print(f\"Df size {len(df)}\")\n",
    "    appendScoresToFile(df, models, path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 45\n",
      " Accuracy of SuccesCommons is 0.8\n",
      "45\n",
      "Df 29size 16\n",
      " Accuracy of SuccessLords is 0.9375\n",
      "61\n",
      "Df 18size 108\n",
      " Accuracy of FailedCommons is 0.6111111111111112\n",
      "169\n",
      "Df 19size 57\n",
      " Accuracy of FailedLords is 0.6666666666666666\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "#### Score the validation and evaluate\n",
    "remaining = [28, 29, 18, 19]\n",
    "accessKeys = { 18: 'FailedCommons', 19: 'FailedLords', 28: \"SuccesCommons\", 29: \"SuccessLords\"}\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\cleanedTexWithID\" \n",
    "\n",
    "\n",
    "nullLordsRate = 559/2416\n",
    "nullCommonsRate = 180/682\n",
    "\n",
    "totalBillsChecked = 0\n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Validation\\\\validation_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False) \n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "    #appendScoresToFile(df, models, path)\n",
    "    dfEvaluate = pd.read_csv(path, error_bad_lines=False) \n",
    "    \n",
    "    dfEvaluate['overall_accuracy'] = None\n",
    "    correct_bills = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        predClass = row['predicted_class']\n",
    "        predCat = accessKeys[i]\n",
    "        if predClass == predCat:\n",
    "            correct_bills += 1\n",
    "            true_positive += 1\n",
    "        if i ==28:\n",
    "            if row['predicted_class'] == \"SuccessLords\":\n",
    "                 correct_bills += 1\n",
    "    total_bills = len(dfEvaluate)\n",
    "    totalBillsChecked += total_bills\n",
    "    accuracy = correct_bills / total_bills\n",
    "    \n",
    "    \n",
    "    print(F\" Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "    print (totalBillsChecked)\n",
    "    \n",
    "    \n",
    "# get all the true positives and true negatives \n",
    "    \n",
    "## For success commons , count success lords and success commons as predicted class and accuracy 0.8, if not 0.26   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 29size 127\n",
      " Accuracy of SuccessLords is 0.968503937007874, nullProb is 13.188327814569536\n",
      "353\n",
      "Df 28size 402\n",
      " Accuracy of SuccesCommons is 0.7810945273631841, nullProb is 13.188327814569536\n",
      "755\n",
      "Df 18size 855\n",
      " Accuracy of FailedCommons is 0.8245614035087719, nullProb is 13.188327814569536\n",
      "1610\n",
      "Df 19size 444\n",
      " Accuracy of FailedLords is 0.8783783783783784, nullProb is 13.188327814569536\n",
      "2054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Training\\\\training_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False) \n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "    #appendScoresToFile(df, models, path)\n",
    "    dfEvaluate = pd.read_csv(path, error_bad_lines=False) \n",
    "    \n",
    "    dfEvaluate['overall_accuracy'] = None\n",
    "    correct_bills = 0\n",
    "    \n",
    "    true_positive = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    true_negative = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        predClass = row['predicted_class']\n",
    "        predCat = accessKeys[i]\n",
    "        if predClass == predCat:\n",
    "            correct_bills += 1\n",
    "            true_positive += 1\n",
    "        if i ==28:\n",
    "            if row['predicted_class'] == \"SuccessLords\":\n",
    "                 correct_bills += 1\n",
    "    total_bills = len(dfEvaluate)\n",
    "    totalBillsChecked += total_bills\n",
    "    accuracy = correct_bills / total_bills\n",
    "    \n",
    "    \n",
    "    print(F\" Accuracy of {accessKeys[i]} is {accuracy}, nullProb is {nullProb}\")\n",
    "    print (totalBillsChecked)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 402\n",
      "Accuracy of SuccesCommons is 0.7810945273631841\n",
      "Df 29size 127\n",
      "Accuracy of SuccessLords is 0.968503937007874\n",
      "Df 18size 855\n",
      "Accuracy of FailedCommons is 0.8245614035087719\n",
      "Df 19size 444\n",
      "Accuracy of FailedLords is 0.8783783783783784\n",
      "Metrics for FailedCommons: Precision: 0.4912891986062718, Recall: 0.4519230769230769, F1 Score: 0.4707846410684474\n",
      "Metrics for FailedLords: Precision: 0.4508670520231214, Recall: 0.4676258992805755, F1 Score: 0.4590935844614479\n",
      "Metrics for SuccesCommons: Precision: 1.0, Recall: 1.0, F1 Score: 1.0\n",
      "Metrics for SuccessLords: Precision: 0.35755813953488375, Recall: 0.492, F1 Score: 0.4141414141414142\n"
     ]
    }
   ],
   "source": [
    "class_metrics = {key: {'TP': 0, 'FP': 0, 'FN': 0} for key in accessKeys}\n",
    "\n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Training\\\\training_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)\n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "\n",
    "\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        predicted_class = row['predicted_class']\n",
    "        actual_class = accessKeys[i]\n",
    "\n",
    "  \n",
    "        if  predicted_class == actual_class:\n",
    "            correct_bills += 1\n",
    "            class_metrics[i]['TP'] += 1\n",
    "        if i == 28:\n",
    "            if predicted_class == \"SuccessLords\":\n",
    "                correct_bills += 1\n",
    "                class_metrics[i]['TP'] += 1\n",
    "        else:\n",
    "            class_metrics[i]['FN'] += 1\n",
    "            if predicted_class in accessKeys.values():  # Check if the prediction is one of the known classes\n",
    "                predicted_key = [k for k, v in accessKeys.items() if v == predicted_class][0]\n",
    "                class_metrics[predicted_key]['FP'] += 1\n",
    "\n",
    "\n",
    "    total_bills = len(df)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    print(F\"Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "\n",
    "for key, metrics in class_metrics.items():\n",
    "    TP = metrics['TP']\n",
    "    FP = metrics['FP']\n",
    "    FN = metrics['FN']\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Metrics for {accessKeys[key]}: Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 45\n",
      "Accuracy of SuccesCommons is 0.8\n",
      "Df 29size 16\n",
      "Accuracy of SuccessLords is 0.9375\n",
      "Df 18size 108\n",
      "Accuracy of FailedCommons is 0.6111111111111112\n",
      "Df 19size 57\n",
      "Accuracy of FailedLords is 0.6666666666666666\n",
      "Metrics for FailedCommons: Precision: 0.46808510638297873, Recall: 0.3793103448275862, F1 Score: 0.41904761904761906\n",
      "Metrics for FailedLords: Precision: 0.38, Recall: 0.4, F1 Score: 0.3897435897435898\n",
      "Metrics for SuccesCommons: Precision: 1.0, Recall: 1.0, F1 Score: 1.0\n",
      "Metrics for SuccessLords: Precision: 0.2542372881355932, Recall: 0.4838709677419355, F1 Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "class_metrics = {key: {'TP': 0, 'FP': 0, 'FN': 0} for key in accessKeys}\n",
    "\n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Validation\\\\validation_text{i}.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)\n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "\n",
    "\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        predicted_class = row['predicted_class']\n",
    "        actual_class = accessKeys[i]\n",
    "\n",
    "  \n",
    "        if  predicted_class == actual_class:\n",
    "            correct_bills += 1\n",
    "            class_metrics[i]['TP'] += 1\n",
    "        if i == 28:\n",
    "            if predicted_class == \"SuccessLords\":\n",
    "                correct_bills += 1\n",
    "                class_metrics[i]['TP'] += 1\n",
    "        else:\n",
    "            class_metrics[i]['FN'] += 1\n",
    "            if predicted_class in accessKeys.values():  # Check if the prediction is one of the known classes\n",
    "                predicted_key = [k for k, v in accessKeys.items() if v == predicted_class][0]\n",
    "                class_metrics[predicted_key]['FP'] += 1\n",
    "\n",
    "\n",
    "    total_bills = len(df)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    print(F\"Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "\n",
    "for key, metrics in class_metrics.items():\n",
    "    TP = metrics['TP']\n",
    "    FP = metrics['FP']\n",
    "    FN = metrics['FN']\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Metrics for {accessKeys[key]}: Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
