{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import expit as sigmoid \n",
    "import os\n",
    "import re\n",
    "import fitz  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Function Definitions \n",
    "\n",
    "# 1 Fail\n",
    "# 2 Success\n",
    "# 8 Commons\n",
    "# 9 Lords\n",
    "#18 failed commons\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitSentences(text):\n",
    "    \n",
    "    sentences = re.split(r'\\.\\s+', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentences.append(filtered_words)\n",
    "        \n",
    "    return filtered_sentences\n",
    "\n",
    "def splitDocument(house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}/Training/training_text{code}.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "# returns a list of lists . List each sentnece, and inside there is a list of words for each sentence\n",
    "\n",
    "\n",
    "def splitDocumentALL(startSession, endSession, house_and_status = [18,19,28,29]):\n",
    "    for code in house_and_status: \n",
    "        file_name = f\"cleanedTextFull/{code}_{startSession} to {endSession} fullText.txt\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    yield splitSentences(line)\n",
    "            print(f\"Read File{file_name}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unicode decode error\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# input (sentence: list of words, model: gensim model, window: window= windowSize of word2vec, \n",
    "#debug: print intermediate calculations for debugging)\n",
    "\n",
    "def score_sentence(sentence, model, window=7, debug=False):\n",
    "    log_prob = 0.0 # total log prob for the sentence\n",
    "    sentence_length = len(sentence)\n",
    "    word_pair_probs = []  \n",
    "\n",
    "    # Code for equation 1 \n",
    "    for index, center_word in enumerate(sentence):\n",
    "        if center_word not in model.wv:\n",
    "            if debug:\n",
    "                print(f\"Center word '{center_word}' not in vocabulary.\")\n",
    "            continue\n",
    "        center_vector = model.wv[center_word]\n",
    "\n",
    "        start = max(0, index - window)\n",
    "        end = min(sentence_length, index + window + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == index:\n",
    "                continue\n",
    "            context_word = sentence[j]\n",
    "            if context_word not in model.wv:\n",
    "                if debug:\n",
    "                    print(f\"Context word '{context_word}' not in vocabulary.\")\n",
    "                continue\n",
    "            context_vector = model.wv[context_word]\n",
    "\n",
    "            dot_product = np.dot(center_vector, context_vector)\n",
    "            prob = sigmoid(dot_product)\n",
    "\n",
    "            word_pair_probs.append((center_word, context_word, prob))\n",
    "\n",
    "            log_prob += np.log(prob + 1e-10)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- Word Pair Probabilities ---\")\n",
    "        for center, context, prob in word_pair_probs:\n",
    "            print(f\"p({context} | {center}) = {prob:.6f}\")\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Score an entire document (S sentences) under all models (Equation 2)\n",
    "# input (sentencces:  a list of sentences ,models: the dictionary of models, window: the window size for score sentences)\n",
    "# outpur: a sentences x categories (failed , succesful ....) with eahc sentence score according to score_sentence\n",
    "\n",
    "def score_document(sentences, models, window=5):\n",
    "    \"\"\"\n",
    "    Compute the score x category matrix of sentence scores for a document.\n",
    "    \n",
    "    sentences: list of sentences, each sentence is a list of words\n",
    "    models: dict of {category: Word2Vec model}\n",
    "    \"\"\"\n",
    "    S = len(sentences)\n",
    "    C = len(models)\n",
    "    \n",
    "    sentence_scores = np.zeros((S, C))\n",
    "    \n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        for c_idx, (category, model) in enumerate(models.items()):\n",
    "            sentence_scores[s_idx, c_idx] = score_sentence(sentence, model, window)\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "# calculate document probabilities (Equation 5)\n",
    "\n",
    "# input: the sxc array\n",
    "# output: a 1x cateories array with the average score for all sentences in document \n",
    "def document_probabilities(sentence_scores):\n",
    "\n",
    "    return sentence_scores.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# compute class probabilities ( Equation 3)\n",
    "\n",
    "# input:  the array from document_probabilities\n",
    "#ouput: normalized probabilities after bayes rule is applied #todo: change the priors to correspond to each class \n",
    "def class_probabilities(log_doc_probs):\n",
    "\n",
    "    num_classes = len(log_doc_probs)\n",
    "    doc_probs = np.exp(log_doc_probs - np.max(log_doc_probs))  \n",
    "    priors = np.ones(num_classes) / num_classes\n",
    "    numerator = doc_probs * priors\n",
    "    denominator = np.sum(numerator)\n",
    "    probs = numerator / denominator\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "# classify the document (Equation 6)\n",
    "# checks which of the numbers in the 1d array from document probabilities (the average across the classes ) is biggest and returns the index and array (for debuging) \n",
    " \n",
    "def classify_document(sentence_scores):\n",
    "    doc_probs = document_probabilities(sentence_scores)\n",
    "    class_probs = class_probabilities(doc_probs)\n",
    "    predicted_class_idx = np.argmax(class_probs)\n",
    "    return predicted_class_idx, class_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained models \n",
    "\n",
    "def scoreSentences(listOfSentences, modelsList, predictedCategory):\n",
    "    success= 0\n",
    "    fail =0\n",
    "    \n",
    "    categories = list(modelsList.keys())\n",
    "    \n",
    "    for i, sentence_text in enumerate(listOfSentences, 1):\n",
    "        document = splitSentences(sentence_text)\n",
    "        sentence_scores = score_document(document, modelsList, window=5)\n",
    "        doc_probs = document_probabilities(sentence_scores)\n",
    "        probs = class_probabilities(doc_probs)\n",
    "        predicted_idx, doc_probs = classify_document(sentence_scores)\n",
    "        \n",
    "        print(f\" Predicting Sentence {document}\")\n",
    "        print(f\"\\nSentence {i}:\")\n",
    "        print(f\"Predicted class: {categories[predicted_idx]}\")\n",
    "        print(f\"Document probabilities: {doc_probs}\")\n",
    "        print(f\"Class probabilities: {probs}\")\n",
    "        \n",
    "        predicted_class = categories[predicted_idx]\n",
    "        \n",
    "        if (predicted_class == predictedCategory or\n",
    "            (predicted_class in [\"FailedCommons\", \"FailedLords\"] and \n",
    "             predictedCategory in [\"FailedCommons\", \"FailedLords\"])):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    total = fail + success\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = success / total\n",
    "        print(f\"Correct Prediction: {accuracy}\")\n",
    "    else:\n",
    "        print(\"Error in predicition.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models and populate models dictionar\n",
    "\n",
    "model_18 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\TitlesModels\\FailedCommons_word2vec_titles.model\")\n",
    "model_19 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\TitlesModels\\FailedLords_word2vec_titles.model\")\n",
    "model_29 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\TitlesModels\\SuccessLords_word2vec_titles.model\")\n",
    "model_28 = Word2Vec.load(r\"C:\\Users\\ander\\Downloads\\MLP\\TitlesModels\\SuccesCommons_word2vec_titles.model\")\n",
    "\n",
    "\n",
    "\n",
    "houseDictionary = {'FailedCommons': [18], 'FailedLords': [19], \"SuccesCommons\": [28], \"SuccessLords\":[29]}\n",
    "\n",
    "models ={}\n",
    "\n",
    "for key, indices in houseDictionary.items():\n",
    "    for index in indices:\n",
    "        if index == 18:\n",
    "            models[key] = model_18\n",
    "        elif index == 19:\n",
    "            models[key] = model_19\n",
    "        elif index == 28:\n",
    "            models[key] = model_28\n",
    "        elif index == 29:\n",
    "            models[key] = model_29\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to compute the scores for the sentences per individual cell and add the whole array \n",
    "#as well as the code for the actual predicted score \n",
    "from tqdm import tqdm\n",
    "def scoreDocumentCSV(text, models):\n",
    "    filtered_sentences = splitSentences(text)\n",
    "    sentence_scores = score_document(filtered_sentences, models, window=7)\n",
    "    index, doc_probs = classify_document(sentence_scores)\n",
    "    \n",
    "    return index, doc_probs\n",
    "        \n",
    "\n",
    "    \n",
    "def appendScoresToFile(df, models, path):\n",
    "    df['predicted_class'] = None\n",
    "    df['doc_probs'] = None\n",
    "    categories = list(models.keys())\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        result_index, probs_array = scoreDocumentCSV(row['Long Title'], models)\n",
    "        \n",
    "        df.at[index, 'predicted_class'] = categories[result_index]\n",
    "        df.at[index, 'doc_probs'] = probs_array\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Into validation and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|████████████████████████████████████████████████████████████████| 58/58 [00:00<00:00, 231.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 572.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 308.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df size 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 344.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "remaining = [28, 29, 18, 19]\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\CSVFilesWithTitleScores\"\n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Testing\\\\{i}_testing.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, on_bad_lines='skip') \n",
    "    df['Long Title'] = df['Long Title'].astype(str) \n",
    "    print(f\"Df size {len(df)}\")\n",
    "    appendScoresToFile(df, models, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 78\n",
      " Accuracy of SuccesCommons is 0.7564102564102564\n",
      "Df 29size 29\n",
      " Accuracy of SuccessLords is 0.9655172413793104\n",
      "Df 18size 243\n",
      " Accuracy of FailedCommons is 0.5925925925925926\n",
      "Df 19size 73\n",
      " Accuracy of FailedLords is 0.6164383561643836\n"
     ]
    }
   ],
   "source": [
    "#### Score the validation and evaluate\n",
    "remaining = [28, 29, 18, 19]\n",
    "accessKeys = { 18: 'FailedCommons', 19: 'FailedLords', 28: \"SuccesCommons\", 29: \"SuccessLords\"}\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\CSVFilesWithTitleScores\"\n",
    "\n",
    "\n",
    "models \n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Validation\\\\{i}_validation.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False) \n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "    #appendScoresToFile(df, models, path)\n",
    "    dfEvaluate = pd.read_csv(path, error_bad_lines=False) \n",
    "    \n",
    "    dfEvaluate['overall_accuracy'] = None\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['predicted_class'] == accessKeys[i]:\n",
    "            correct_bills += 1\n",
    "        if i == 18:\n",
    "            if row['predicted_class'] == \"FailedLords\":\n",
    "                correct_bills += 1\n",
    "    total_bills = len(dfEvaluate)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    \n",
    "    print(F\" Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\miniconda3\\envs\\pdiot\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 465\n",
      " Accuracy of SuccesCommons is 0.621505376344086\n",
      "Df 29size 149\n",
      " Accuracy of SuccessLords is 0.7449664429530202\n",
      "Df 18size 1938\n",
      " Accuracy of FailedCommons is 0.567079463364293\n",
      "Df 19size 550\n",
      " Accuracy of FailedLords is 0.5872727272727273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Training\\\\{i}_training.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False) \n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "    #appendScoresToFile(df, models, path)\n",
    "    dfEvaluate = pd.read_csv(path, error_bad_lines=False) \n",
    "    \n",
    "    dfEvaluate['overall_accuracy'] = None\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row['predicted_class'] == accessKeys[i]:\n",
    "            correct_bills += 1\n",
    "        if i == 18:\n",
    "            if row['predicted_class'] == \"FailedLords\":\n",
    "                correct_bills += 1\n",
    "    total_bills = len(dfEvaluate)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    \n",
    "    print(F\" Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 465\n",
      "Accuracy of SuccesCommons is 0.621505376344086\n",
      "Df 29size 149\n",
      "Accuracy of SuccessLords is 0.7449664429530202\n",
      "Df 18size 1938\n",
      "Accuracy of FailedCommons is 0.567079463364293\n",
      "Df 19size 550\n",
      "Accuracy of FailedLords is 0.5872727272727273\n",
      "Metrics for FailedCommons: Precision: 0.9105219552609777, Recall: 1.0, F1 Score: 0.95316565481353\n",
      "Metrics for FailedLords: Precision: 0.4581560283687943, Recall: 0.3699885452462772, F1 Score: 0.4093789607097592\n",
      "Metrics for SuccesCommons: Precision: 0.43655589123867067, Recall: 0.383289124668435, F1 Score: 0.4081920903954802\n",
      "Metrics for SuccessLords: Precision: 0.26941747572815533, Recall: 0.4269230769230769, F1 Score: 0.3303571428571428\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "remaining = [28, 29, 18, 19]\n",
    "accessKeys = {18: 'FailedCommons', 19: 'FailedLords', 28: \"SuccesCommons\", 29: \"SuccessLords\"}\n",
    "basePath = r\"C:\\Users\\ander\\Downloads\\MLP\\CSVFilesWithTitleScores\"\n",
    "\n",
    "\n",
    "class_metrics = {key: {'TP': 0, 'FP': 0, 'FN': 0} for key in accessKeys}\n",
    "\n",
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Training\\\\{i}_training.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)\n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "\n",
    "\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        predicted_class = row['predicted_class']\n",
    "        actual_class = accessKeys[i]\n",
    "\n",
    "  \n",
    "        if  predicted_class == actual_class:\n",
    "            correct_bills += 1\n",
    "            class_metrics[i]['TP'] += 1\n",
    "        if i == 18:\n",
    "            if predicted_class == \"FailedLords\":\n",
    "                correct_bills += 1\n",
    "                class_metrics[i]['TP'] += 1\n",
    "        else:\n",
    "            class_metrics[i]['FN'] += 1\n",
    "            if predicted_class in accessKeys.values():  # Check if the prediction is one of the known classes\n",
    "                predicted_key = [k for k, v in accessKeys.items() if v == predicted_class][0]\n",
    "                class_metrics[predicted_key]['FP'] += 1\n",
    "\n",
    "\n",
    "    total_bills = len(df)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    print(F\"Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "\n",
    "for key, metrics in class_metrics.items():\n",
    "    TP = metrics['TP']\n",
    "    FP = metrics['FP']\n",
    "    FN = metrics['FN']\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Metrics for {accessKeys[key]}: Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 28size 78\n",
      "Accuracy of SuccesCommons is 0.7564102564102564\n",
      "Df 29size 29\n",
      "Accuracy of SuccessLords is 0.9655172413793104\n",
      "Df 18size 243\n",
      "Accuracy of FailedCommons is 0.5925925925925926\n",
      "Df 19size 73\n",
      "Accuracy of FailedLords is 0.6164383561643836\n",
      "Metrics for FailedCommons: Precision: 0.9139705882352941, Recall: 1.0, F1 Score: 0.9550518632347291\n",
      "Metrics for FailedLords: Precision: 0.45714285714285713, Recall: 0.3713420787083754, F1 Score: 0.40979955456570155\n",
      "Metrics for SuccesCommons: Precision: 0.4393939393939394, Recall: 0.39057239057239057, F1 Score: 0.4135472370766488\n",
      "Metrics for SuccessLords: Precision: 0.2865979381443299, Recall: 0.4384858044164038, F1 Score: 0.3466334164588528\n"
     ]
    }
   ],
   "source": [
    "for i in remaining:\n",
    "    filePath = f\"{i}\\\\Validation\\\\{i}_validation.csv\"\n",
    "    path = os.path.join(basePath, filePath)\n",
    "    df = pd.read_csv(path, error_bad_lines=False)\n",
    "    print(f\"Df {i}size {len(df)}\")\n",
    "\n",
    "\n",
    "    correct_bills = 0\n",
    "    for index, row in df.iterrows():\n",
    "        predicted_class = row['predicted_class']\n",
    "        actual_class = accessKeys[i]\n",
    "\n",
    "  \n",
    "        if  predicted_class == actual_class:\n",
    "            correct_bills += 1\n",
    "            class_metrics[i]['TP'] += 1\n",
    "        if i == 18:\n",
    "            if predicted_class == \"FailedLords\":\n",
    "                correct_bills += 1\n",
    "                class_metrics[i]['TP'] += 1\n",
    "        else:\n",
    "            class_metrics[i]['FN'] += 1\n",
    "            if predicted_class in accessKeys.values():  # Check if the prediction is one of the known classes\n",
    "                predicted_key = [k for k, v in accessKeys.items() if v == predicted_class][0]\n",
    "                class_metrics[predicted_key]['FP'] += 1\n",
    "\n",
    "\n",
    "    total_bills = len(df)\n",
    "    accuracy = correct_bills / total_bills\n",
    "    print(F\"Accuracy of {accessKeys[i]} is {accuracy}\")\n",
    "\n",
    "for key, metrics in class_metrics.items():\n",
    "    TP = metrics['TP']\n",
    "    FP = metrics['FP']\n",
    "    FN = metrics['FN']\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Metrics for {accessKeys[key]}: Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
